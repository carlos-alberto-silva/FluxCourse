{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fluxcourse SIF section (2)\n",
    "\n",
    "## Introduction\n",
    "In this section, we will learn how to process SIF data from multiple satellite platforms. At the end of this section, we will compare satellite SIF data with GPP estimation from a few FLUXNET towers. We will go through the following sections:\n",
    "\n",
    "* The start: how to read SIF data?\n",
    "* Post-processing: how to create scientifically useful datasets and visualize them?\n",
    "* Do what we see from the satellite agree with what we see from the ground (i.e. FLUXNET or Ameriflux)?\n",
    "\n",
    "## Logistics\n",
    "We use Jupyter Notebook for this practice. Remember to install the packages using \"conda install [package-name]\" in your command line. Before the course, remember to update your conda (if you use anaconda) by \"conda update conda\".\n",
    "You will need the following packages:\n",
    "\n",
    "* matplotlib\n",
    "* numpy\n",
    "* scipy\n",
    "* h5py\n",
    "* netCDF4\n",
    "* glob\n",
    "* traceback\n",
    "\n",
    "A large portion of this code is from **Christian Frankenberg and Philipp Koehler**. L2_tools.py provides the tools to use TROPOMI and OCO-2 L2 data. You will need to have the following files under the same folder of this code:\n",
    "\n",
    "* L2_tools.py: read SIF data; calculate running mean\n",
    "* spectral_shape.dat: the spectral shape you created from the previous section\n",
    "\n",
    "Another useful tool to read netCDF file is Panoply. It is a free software from NASA. Download it from here: https://www.giss.nasa.gov/tools/panoply/\n",
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step, instead of writing python code, is to read these satellite data with Panoply. Panoply provides a graphical-user-interface (GUI) that allows you to check the variables available and visualize them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's load a few packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to install these packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import h5py\n",
    "# nicer figures using ggg plot style.\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import clear_output\n",
    "from IPython.core.pylabtools import figsize\n",
    "\n",
    "# Register the converters to avoid further warnings\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "# hide all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember to add L2_tool.py to the same folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file L2_tools.py is in the same folder here and will provide easy access to TROPOMI and OCO-2 L2 data \n",
    "from L2_tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add the paths for OCO2 and TROPOMI**\n",
    "Here we use ungridded datasets, meaning individual soundings of satellite scans. We will select a geographical region, and average all the data points in that region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please adapt to your local directories after having mirrored our ftp data (and extracted the ungridded TROPOMI files):\n",
    "path_tropomi = '/Volumes/PERS_airbor/FluxCourse/TROPOMI/nc_ungridded/'\n",
    "path_oco2 = '/Volumes/PERS_airbor/FluxCourse/OCO2/sif_lite_B8100/'\n",
    "\n",
    "# This is still cumbersome, the units are part of the netCDF4 files but I (CF) hardcode them for now:\n",
    "t_unit_oco2    = \"seconds since 1993-01-01 00:00:00\"\n",
    "t_unit_tropomi = \"seconds since 1970-01-01 00:00:00\"\n",
    "t_cal = u\"gregorian\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What will something that does not fluoresce look like in the satellite SIF images?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us read in some OCO-2 and TROPOMI data over a non-fluorescing region in the Sahara\n",
    "# Define region boundaries for a part of the Sahara:\n",
    "latMin = 20\n",
    "latMax = 23\n",
    "lonMin = 5\n",
    "lonMax = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, depending on how much data you have, the following two functions may take some time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The syntax for reading in data is as follows: data = L2(path_to_data, dictionary,latMin, latMax, lonMin, lonMax ) -- the last four input to the function define the rectangle of the region of interest\n",
    "\n",
    "Find some examples for the dictionary below (these can be adapted to read in GOME-2 data). The format describes the \"key\" as how you want to name the variable in your returned class and the \"item\" as the variable name of the HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oco2_sahara = L2(path_oco2+'2018/0[45678]/*.nc4', dict_oco2,latMin, latMax, lonMin, lonMax )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TROPOMI will take a bit more time, be patient!\n",
    "tropomi_sahara = L2(path_tropomi+'*2018-0[45678]*.nc', dict_tropomi,latMin, latMax, lonMin, lonMax )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It basically goes through the HDF5 files and uses a dictionary to decide which variables to read in. Then, it just concatenates all the data into a larger set and stores everything in a class (just think of it like a structure for now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can take a look at L2_tools.py, and you will find the following code, which are the variables in OCO2 or TROPOMI files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_oco2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_tropomi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we have the data loaded, let's plot some figures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First convert oco2 time units to a python time (this is a clunky step right now, need to automate later!)\n",
    "oco2_sahara.time_python    = convert_time(oco2_sahara.time, t_unit_oco2, t_cal)\n",
    "tropomi_sahara.time_python = convert_time(tropomi_sahara.time, t_unit_tropomi, t_cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets plot a timeseries\n",
    "figsize(12,6)\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(oco2_sahara.time_python, oco2_sahara.sif_757,'.')\n",
    "plt.ylabel('SIF (W/m$^2$/sr/$\\mu$m)')\n",
    "plt.title('OCO2 (SIF at 758nm)')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(tropomi_sahara.time_python, tropomi_sahara.sif,'.')\n",
    "plt.ylabel('SIF (W/m$^2$/sr/$\\mu$m)')\n",
    "plt.title('TROPOMI (SIF at 740nm)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hmmm, wait a minute! Does the result make sense?**\n",
    "\n",
    "We looked at the Sahara and SIF should be around 0. The data looks very noisy, can we make use of that? Should we just exclude all the negative data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let us look at a histogram\n",
    "plt.hist(oco2_sahara.sif_757, 50,range=(-3,3), density=True, alpha=0.5, label='OCO-2')\n",
    "plt.hist(tropomi_sahara.sif,  50,range=(-3,3), density=True, alpha=0.5, label='TROPOMI')\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Standard deviation of measured OCO-2 SIF data in Sahara ' + str(np.std(oco2_sahara.sif_757)) )\n",
    "print('Expected standard deviation based on posteriori error ' + str(np.mean(oco2_sahara.sif_757_sigma)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean if we exclude negative data ' + str(np.mean(oco2_sahara.sif_757[oco2_sahara.sif_757>=0])))\n",
    "print('True mean ' + str(np.mean(oco2_sahara.sif_757)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see 2 things here: a) removing negative values is dangerous. If the true value is 0 and we have measurement noise, negative retrievals are not unphysical if they are within the posterior noise estimate; b) we sometimes have averages that are below  (e.g. -0.12 here). This can happen and really depends on the posterior bias correction. \n",
    "\n",
    "Christian and Philipp are still working on this but removing small biases on the order of 0.1 is really hard.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's take a look at some vegetated surface**\n",
    "\n",
    "What would you expect to see? Say from April 2018 to August 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define region boundaries (roughly Iowa)\n",
    "latMin_cb = 40\n",
    "latMax_cb = 43\n",
    "lonMin_cb = -96\n",
    "lonMax_cb = -91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oco2_iowa = L2(path_oco2+'2018/0[45678]/*.nc4', dict_oco2,latMin_cb, latMax_cb, lonMin_cb, lonMax_cb )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tropomi_iowa = L2(path_tropomi+'*2018-0[45678]*.nc', dict_tropomi,latMin_cb, latMax_cb, lonMin_cb, lonMax_cb )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, convert tedious times in the files to something you can work with in python\n",
    "oco2_iowa.time_python    = convert_time(oco2_iowa.time, t_unit_oco2, t_cal)\n",
    "tropomi_iowa.time_python = convert_time(tropomi_iowa.time, t_unit_tropomi, t_cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets plot a timeseries\n",
    "figsize(14,6)\n",
    "plt.subplot(1,2,1)\n",
    "nadir = np.where(oco2_iowa.mode==0)[0]\n",
    "glint = np.where(oco2_iowa.mode==1)[0]\n",
    "plt.plot(oco2_iowa.time_python[nadir], oco2_iowa.sif_757[nadir],'.',markersize=0.5, label='Nadir')\n",
    "plt.plot(oco2_iowa.time_python[glint], oco2_iowa.sif_757[glint],'.',markersize=0.5, label='Glint')\n",
    "plt.ylabel('SIF (W/m$^2$/sr/$\\mu$m)')\n",
    "plt.title('OCO2 (SIF at 758nm)')\n",
    "plt.legend(loc=0)\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(tropomi_iowa.time_python, tropomi_iowa.sif,'.',markersize=0.5)\n",
    "plt.ylabel('SIF (W/m$^2$/sr/$\\mu$m)')\n",
    "plt.title('TROPOMI (SIF at 740nm)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you see here?**\n",
    "A few things: 1) Both OCO2 and TROPOMI show a similar seasonal pattern; 2) Quite large variations each day. 3) Glint and Nadir view produce similar values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Let us compute timeseries from OCO-2 and TROPOMI data\n",
    "You can see that individual data are too noisy to work with, thus we have to average. Looking at Iowa, we see that we have more than enough data for averaging. Now we can define a temporal resolution and create a running mean to drive down the noise (which should scale with $1/\\sqrt{n}$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us define some averaging interval, get data every 3 days and create a running mean of +/- 3 days:\n",
    "from datetime import datetime\n",
    "# Define data range to smooth on (every 3 days)\n",
    "dates = np.arange('2018-04', '2018-08', dtype='datetime64[3D]').astype(datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We used dates with a 3 day time-step and now use a +/- 3 day filter for the running mean:\n",
    "iowa_timeseries_oco2 =    sif_rMean(oco2_iowa.time_python, oco2_iowa.sif_757,dates, 3 )\n",
    "iowa_timeseries_tropomi = sif_rMean(tropomi_iowa.time_python, tropomi_iowa.sif,dates, 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can play with the time-steps to create something like a 30-day running mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion factor from 758nm to 740nm is roughly 1.55\n",
    "cf = 1.55\n",
    "plt.errorbar(dates, iowa_timeseries_oco2.mean,yerr=iowa_timeseries_oco2.standard_error, label='OCO-2 Mean')\n",
    "plt.errorbar(dates, iowa_timeseries_tropomi.mean,yerr=iowa_timeseries_tropomi.standard_error,  label='TROPOMI Mean')\n",
    "plt.ylabel('SIF (W/m$^2$/sr/$\\mu$m)')\n",
    "plt.legend(loc=0)\n",
    "plt.title('Iowa Timeseries, +/-3 day running mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We see that the time-series agree reasonably well but that there is an offset (or scaling factor) as we define SIF at different wavelength. Why? (Hint: think about the leaf SIF measurements you made).\n",
    "\n",
    "I have loaded some SIF spectra based on SCOPE and leaf-level measurements using L2_tools.\n",
    "\n",
    "* shape_scope is the SIF spectra from SCOPE\n",
    "* shape_walz is the SIF spectra from Magney et al. (2017) NPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(shape_scope[:,0], shape_scope[:,1], label='SCOPE')\n",
    "plt.plot(shape_walz[:,0], shape_walz[:,1]*12, label='Leaf level measurements')\n",
    "plt.title('normalized SIF shape')\n",
    "plt.xlabel('Wavelength (nm)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that SIF changes quite a bit with wavelength but that the overall shape $>$740nm us pretty consistent between our leaf level measurements and the SCOPE model. The shorter wavelength is more affected by chlorophyll re-absorption, which can dramatically change the shape. Read Troy's paper here: \n",
    "\n",
    "Magney, T.S., Frankenberg, C., Fisher, J.B., Sun, Y., North, G.B., Davis, T.S., Kornfeld, A. and Siebke, K., 2017. [Connecting active to passive fluorescence with photosynthesis: A method for evaluating remote sensing measurements of Chl fluorescence.](https://doi.org/10.1111/nph.14662) New Phytologist, 215(4), pp.1594-1608.\n",
    "\n",
    "### How to correct for it?\n",
    "We provide a simple routine to convert the scaling factor needed to convert data from wavelength X to Y: Let's say we want to convert OCO-2 data from 758nm to the TROPOMI grid of 740nm, we get the following scaling factor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Factor from leaf level ',convertWL(758,740,shape_walz))\n",
    "fac = convertWL(758,740,shape_walz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the factor to better match the 2 time-series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion factor from 758nm to 740nm is roughly 1.55 (we can provide shapes if needed)\n",
    "plt.errorbar(dates, iowa_timeseries_oco2.mean*fac,yerr=iowa_timeseries_oco2.standard_error*fac, label='OCO-2 Mean')\n",
    "plt.errorbar(dates, iowa_timeseries_tropomi.mean,yerr=iowa_timeseries_tropomi.standard_error,  label='TROPOMI Mean')\n",
    "plt.ylabel('SIF @740nm (W/m$^2$/sr/$\\mu$m)')\n",
    "plt.legend(loc=0)\n",
    "plt.title('Iowa Timeseries, +/-3 day running mean')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correction for overpass time and length of day:\n",
    "Another crucial impact is due to changes in the overpass time and how to relate the instantaenous SIF to a daily average. Our first simple approximations are just based on calculations of the ratio of a daily average PAR vs instantaenous PAR (assuming cloud free conditions). To first order, this should bring different instruments into better alignment. We provide this conversion factor for all instruments. For TROPOMI and OCO-2, it is not a huge factor as the overpass time is similar (note though the the factor changes between the eastern and western part of the TROPOMI swath as the local time changes quite a bit):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the SIF timeseries including the length of day correction:\n",
    "# Do the same for the length of day correction\n",
    "iowa_timeseries_tropomiDC = sif_rMean(tropomi_iowa.time_python, tropomi_iowa.sif*tropomi_iowa.dcCorr,dates, 3 )\n",
    "iowa_timeseries_oco2DC =    sif_rMean(oco2_iowa.time_python, oco2_iowa.sif_757*oco2_iowa.dcCorr,dates, 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dates, iowa_timeseries_oco2DC.mean*fac, label='OCO-2 LOD corrected')\n",
    "plt.plot(dates, iowa_timeseries_tropomiDC.mean, label='TROPOMI LOD corrected')\n",
    "plt.legend()\n",
    "plt.ylabel('$\\overline{SIF}$ @740nm (W/m$^2$/sr/$\\mu$m)')\n",
    "plt.title('Length of Day correction impact')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the temporal trend and the magnitude of SIF values you get, what do you see? Why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's do it for another site. How about do it yourself? Let's choose Tonzi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's define the Region of Interest\n",
    "latMin_cb = 38.4\n",
    "latMax_cb = 38.5\n",
    "lonMin_cb = -121\n",
    "lonMax_cb = -120.9\n",
    "\n",
    "# Read OCO2 and TROPOMI data\n",
    "#oco2_tonzi = L2(path_oco2+'2018/0[45678]/*.nc4', dict_oco2,latMin_cb, latMax_cb, lonMin_cb, lonMax_cb )\n",
    "tropomi_tonzi = L2(path_tropomi+'*.nc', dict_tropomi,latMin_cb, latMax_cb, lonMin_cb, lonMax_cb )\n",
    "\n",
    "# Convert time\n",
    "#oco2_tonzi.time_python    = convert_time(oco2_tonzi.time, t_unit_oco2, t_cal)\n",
    "tropomi_tonzi.time_python = convert_time(tropomi_tonzi.time, t_unit_tropomi, t_cal)\n",
    "\n",
    "# Just do the averaging\n",
    "from datetime import datetime\n",
    "# Define data range to smooth on (every 3 days)\n",
    "dates = np.arange('2018-04', '2019-04', dtype='datetime64[3D]').astype(datetime)\n",
    "\n",
    "# Let's calculate the SIF timeseries including the length of day correction:\n",
    "# Do the same for the length of day correction\n",
    "tonzi_timeseries_tropomiDC = sif_rMean(tropomi_tonzi.time_python, tropomi_tonzi.sif*tropomi_tonzi.dcCorr,dates, 3 )\n",
    "#tonzi_timeseries_oco2DC =    sif_rMean(oco2_tonzi.time_python, oco2_tonzi.sif_757*oco2_tonzi.dcCorr,dates, 3 )\n",
    "\n",
    "# Here we will use the data you create from leaf-level measurements\n",
    "fac = convertWL(758,740,shape_walz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How good it is compared with EC data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path      = '/Users/xiyang/Documents/GitHub/FluxCourse/Tonzi_DailyGEE2.csv'\n",
    "df        = pd.read_csv(path)\n",
    "t_ec_unit = 'days since 1900-01-01 00:00:00'\n",
    "ecdates   = num2date(df['DOY'],units = t_ec_unit,calendar = t_cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "#ax1.plot(dates, tonzi_timeseries_oco2DC.mean*fac, label='OCO-2 LOD corrected')\n",
    "ax1.plot(dates, tonzi_timeseries_tropomiDC.mean,'r-', label='TROPOMI LOD corrected')\n",
    "ax1.set_xlim([datetime(2018,1,1,0,0),datetime(2018,12,31,0,0)])\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(ecdates, -df['GEE'],'b-')\n",
    "ax2.set_xlim([datetime(2018,1,1,0,0),datetime(2018,12,31,0,0)])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
